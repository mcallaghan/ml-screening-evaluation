@article{abbott_neural_nodate,
  title = {Neural {{Circuit Diagrams}}: {{Robust Diagrams}} for the {{Communication}}, {{Implementation}}, and {{Analysis}} of {{Deep Learning Architectures}}},
  author = {Abbott, Vincent},
  abstract = {Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation.},
  langid = {english},
  file = {/home/max/Zotero/storage/FVJSFWQM/Abbott - Neural Circuit Diagrams Robust Diagrams for the C.pdf}
}

@misc{biderman_pythia_2023,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and {van der Wal}, Oskar},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01373},
  eprint = {2304.01373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.01373},
  urldate = {2023-04-25},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/F2RCSXS7/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf;/home/max/Zotero/storage/LJ8QPND4/2304.html}
}

@misc{bubeck_sparks_2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2023-04-25},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/854A6GWZ/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf;/home/max/Zotero/storage/LPPV3KQF/2303.html}
}

@article{callaghan_statistical_2020,
  title = {Statistical {{Stopping Criteria}} for {{Automated Screening}} in {{Systematic Reviews}}},
  author = {Callaghan, Max and {M{\"u}ller-Hansen}, Finn},
  year = {2020},
  month = sep,
  journal = {Systematic Reviews},
  doi = {10.21203/rs.2.18218/v2},
  urldate = {2020-11-25},
  abstract = {Active learning for systematic review screening promises to reduce the human effort required to identify relevant documents for a systematic review.~Machines and humans~ work together, with humans providing training data, and the machine optimising the documents that the humans screen. This enabl...},
  langid = {english},
  file = {/home/max/Zotero/storage/77BJSLM2/2020 - Statistical Stopping Criteria for Automated Screen.pdf}
}

@misc{de_bruin_synergy_2023,
  title = {{{SYNERGY}} - {{Open}} Machine Learning Dataset on Study Selection in Systematic Reviews},
  author = {De Bruin, Jonathan and Ma, Yongchao and Ferdinands, Gerbrich and Teijema, Jelle and {Van de Schoot}, Rens},
  year = {2023},
  month = apr,
  publisher = {DataverseNL},
  doi = {10.34894/HE6NAQ},
  urldate = {2024-09-05},
  abstract = {SYNERGY is a free and open dataset on study selection in systematic reviews, comprising 169,288 academic works from 26 systematic reviews. Only 2,834 (1.67\%) of the academic works in the binary classified dataset are included in the systematic reviews. This makes the SYNERGY dataset a unique dataset for the development of information retrieval algorithms, especially for sparse labels. Due to the many available variables available per record (i.e. titles, abstracts, authors, references, topics), this dataset is useful for researchers in NLP, machine learning, network analysis, and more. In total, the dataset contains 82,668,134 trainable data points. The easiest way to get the SYNERGY dataset is via the synergy-dataset Python package. See https://github.com/asreview/synergy-dataset for all information.},
  copyright = {http://creativecommons.org/publicdomain/zero/1.0},
  langid = {english},
  keywords = {Computer and Information Science,Engineering,Health and Life Sciences,Medicine,Social Sciences},
  file = {/home/max/Zotero/storage/3EANYYSI/dataset.html}
}

@misc{devlin_bert_2019-1,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-04-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/RK8W6ZR2/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/max/Zotero/storage/KB926TY9/1810.html}
}

@misc{feng_cook_2023,
  title = {{{CooK}}: {{Empowering General-Purpose Language Models}} with {{Modular}} and {{Collaborative Knowledge}}},
  shorttitle = {{{CooK}}},
  author = {Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  year = {2023},
  month = may,
  number = {arXiv:2305.09955},
  eprint = {2305.09955},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-14},
  abstract = {Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select and retain information in generated documents by controlling for relevance, brevity, and factuality. Finally, we propose bottom-up and top-down knowledge integration approaches to augment general-purpose LLMs with the curated (relevant, factual) knowledge from community-driven specialized LMs that enable multi-domain knowledge synthesis and on-demand knowledge requests. Through extensive experiments, we demonstrate that CooK achieves state-of-the-art performance on six benchmark datasets. Our results highlight the potential of enriching general-purpose LLMs with evolving and modular knowledge -- relevant knowledge that can be continuously updated through the collective efforts of the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{ganguli_predictability_2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = {2022},
  month = jun,
  eprint = {2202.07785},
  primaryclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  urldate = {2023-05-02},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/home/max/Zotero/storage/ZYFLIJRM/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf;/home/max/Zotero/storage/7E87S2NF/2202.html}
}

@article{kapoor_leakage_2023,
  title = {Leakage and the Reproducibility Crisis in Machine-Learning-Based Science},
  author = {Kapoor, Sayash and Narayanan, Arvind},
  year = {2023},
  month = sep,
  journal = {Patterns},
  volume = {4},
  number = {9},
  pages = {100804},
  issn = {26663899},
  doi = {10.1016/j.patter.2023.100804},
  urldate = {2023-11-15},
  abstract = {Machine-learning (ML) methods have gained prominence in the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. We systematically investigate reproducibility issues in ML-based science. Through a survey of literature in fields that have adopted ML methods, we find 17 fields where leakage has been found, collectively affecting 294 papers and, in some cases, leading to wildly overoptimistic conclusions. Based on our survey, we introduce a detailed taxonomy of eight types of leakage, ranging from textbook errors to open research problems. We propose that researchers test for each type of leakage by filling out model info sheets, which we introduce. Finally, we conduct a reproducibility study of civil war prediction, where complex ML models are believed to vastly outperform traditional statistical models such as logistic regression (LR). When the errors are corrected, complex ML models do not perform substantively better than decades-old LR models.},
  langid = {english},
  file = {/home/max/Zotero/storage/HZ382PY2/Kapoor and Narayanan - 2023 - Leakage and the reproducibility crisis in machine-.pdf}
}

@misc{kristensen-mclachlan_chatbots_2023,
  title = {Chatbots {{Are Not Reliable Text Annotators}}},
  author = {{Kristensen-McLachlan}, Ross Deans and Canavan, Miceal and Kardos, M{\'a}rton and Jacobsen, Mia and Aar{\o}e, Lene},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05769},
  eprint = {2311.05769},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.05769},
  urldate = {2023-11-14},
  abstract = {Recent research highlights the significant potential of ChatGPT for text annotation in social science research. However, ChatGPT is a closed-source product which has major drawbacks with regards to transparency, reproducibility, cost, and data protection. Recent advances in open-source (OS) large language models (LLMs) offer alternatives which remedy these challenges. This means that it is important to evaluate the performance of OS LLMs relative to ChatGPT and standard approaches to supervised machine learning classification. We conduct a systematic comparative evaluation of the performance of a range of OS LLM models alongside ChatGPT, using both zero- and few-shot learning as well as generic and custom prompts, with results compared to more traditional supervised classification models. Using a new dataset of Tweets from US news media, and focusing on simple binary text annotation tasks for standard social science concepts, we find significant variation in the performance of ChatGPT and OS models across the tasks, and that supervised classifiers consistently outperform both. Given the unreliable performance of ChatGPT and the significant challenges it poses to Open Science we advise against using ChatGPT for substantive text annotation tasks in social science research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/GUWKDXR7/Kristensen-McLachlan et al. - 2023 - Chatbots Are Not Reliable Text Annotators.pdf;/home/max/Zotero/storage/KEPA88DZ/2311.html}
}

@misc{liesenfeld_opening_2023,
  title = {Opening up {{ChatGPT}}: {{Tracking}} Openness, Transparency, and Accountability in Instruction-Tuned Text Generators},
  shorttitle = {Opening up {{ChatGPT}}},
  author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  year = {2023},
  month = jul,
  eprint = {2307.05532},
  primaryclass = {cs},
  doi = {10.1145/3571884.3604316},
  urldate = {2023-07-17},
  abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/5DGNQCL4/Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf;/home/max/Zotero/storage/KB7C8KRG/2307.html}
}

@misc{liu_pre-train_2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  number = {arXiv:2107.13586},
  eprint = {2107.13586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.13586},
  urldate = {2024-05-22},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/2JCUHP9I/Liu et al. - 2021 - Pre-train, Prompt, and Predict A Systematic Surve.pdf;/home/max/Zotero/storage/8W6JWB7D/2107.html}
}

@misc{liu_roberta_2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2023-04-25},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/3VWNQC5H/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/home/max/Zotero/storage/W68JPINJ/1907.html}
}

@misc{minaee_large_2024,
  title = {Large {{Language Models}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}}},
  author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06196},
  eprint = {2402.06196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06196},
  urldate = {2024-03-06},
  abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{ouyang_training_2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2023-04-25},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/B556KYKG/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/home/max/Zotero/storage/6F65RXP6/2203.html}
}

@misc{parr_matrix_2018,
  title = {The {{Matrix Calculus You Need For Deep Learning}}},
  author = {Parr, Terence and Howard, Jeremy},
  year = {2018},
  month = jul,
  number = {arXiv:1802.01528},
  eprint = {1802.01528},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.01528},
  urldate = {2023-07-13},
  abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/MXHDB2ZP/Parr and Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf;/home/max/Zotero/storage/9C872MSM/1802.html}
}

@misc{schaeffer_are_2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.15004},
  urldate = {2023-05-02},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/JIJWR4K9/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf;/home/max/Zotero/storage/WGJNGQXY/2304.html}
}

@misc{shumailov_curse_2023,
  title = {The {{Curse}} of {{Recursion}}: {{Training}} on {{Generated Data Makes Models Forget}}},
  shorttitle = {The {{Curse}} of {{Recursion}}},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  year = {2023},
  month = may,
  number = {arXiv:2305.17493},
  eprint = {2305.17493},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17493},
  urldate = {2023-06-14},
  abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/YTWRNU4F/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data.pdf;/home/max/Zotero/storage/VF272J4N/2305.html}
}

@article{spirling_why_2023,
  title = {Why Open-Source Generative {{AI}} Models Are an Ethical Way Forward for Science},
  author = {Spirling, Arthur},
  year = {2023},
  month = apr,
  journal = {Nature},
  volume = {616},
  number = {7957},
  pages = {413--413},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-023-01295-4},
  urldate = {2023-04-26},
  abstract = {Researchers should avoid the lure of proprietary models and develop transparent large language models to ensure reproducibility.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Ethics,Machine learning,Scientific community,Technology},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: World View\\
Subject\_term: Ethics, Machine learning, Technology, Scientific community},
  file = {/home/max/Zotero/storage/NKVXXVQ6/Spirling - 2023 - Why open-source generative AI models are an ethica.pdf;/home/max/Zotero/storage/CIAFKBDA/d41586-023-01295-4.html}
}

@misc{syriani_assessing_2023,
  title = {Assessing the {{Ability}} of {{ChatGPT}} to {{Screen Articles}} for {{Systematic Reviews}}},
  author = {Syriani, Eugene and David, Istvan and Kumar, Gauransh},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06464},
  eprint = {2307.06464},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.06464},
  urldate = {2024-01-30},
  abstract = {By organizing knowledge within a research field, Systematic Reviews (SR) provide valuable leads to steer research. Evidence suggests that SRs have become first-class artifacts in software engineering. However, the tedious manual effort associated with the screening phase of SRs renders these studies a costly and error-prone endeavor. While screening has traditionally been considered not amenable to automation, the advent of generative AI-driven chatbots, backed with large language models is set to disrupt the field. In this report, we propose an approach to leverage these novel technological developments for automating the screening of SRs. We assess the consistency, classification performance, and generalizability of ChatGPT in screening articles for SRs and compare these figures with those of traditional classifiers used in SR automation. Our results indicate that ChatGPT is a viable option to automate the SR processes, but requires careful considerations from developers when integrating ChatGPT into their SR tools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Software Engineering},
  file = {/home/max/Zotero/storage/4F47RWCE/Syriani et al. - 2023 - Assessing the Ability of ChatGPT to Screen Article.pdf;/home/max/Zotero/storage/2E8PY8YT/2307.html}
}

@misc{turc_well-read_2019,
  title = {Well-{{Read Students Learn Better}}: {{On}} the {{Importance}} of {{Pre-training Compact Models}}},
  shorttitle = {Well-{{Read Students Learn Better}}},
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = sep,
  number = {arXiv:1908.08962},
  eprint = {1908.08962},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.08962},
  urldate = {2023-06-15},
  abstract = {Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/GUV6N8TV/Turc et al. - 2019 - Well-Read Students Learn Better On the Importance.pdf;/home/max/Zotero/storage/KL9UJABB/1908.html}
}

@misc{wang_large_2024,
  title = {Large Language Models Cannot Replace Human Participants Because They Cannot Portray Identity Groups},
  author = {Wang, Angelina and Morgenstern, Jamie and Dickerson, John P.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01908},
  eprint = {2402.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01908},
  urldate = {2024-04-22},
  abstract = {Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains -- including as replacements for human participants in computational social science, user testing, annotation tasks, and more. Traditionally, in all of these settings survey distributors are careful to find representative samples of the human population to ensure the validity of their results and understand potential demographic differences. This means in order to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (i.e., relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are doomed to both misportray and flatten the representations of demographic groups, then empirically show this to be true on 4 LLMs through a series of human studies with 3200 participants across 16 demographic identities. We also discuss a third consideration about how identity prompts can essentialize identities. Throughout, we connect each of these limitations to a pernicious history that shows why each is harmful for marginalized demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the goal is to supplement rather than replace (e.g., pilot studies), we provide empirically-better inference-time techniques to reduce, but not remove, these harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@misc{wang_large_2024-1,
  title = {Large Language Models Cannot Replace Human Participants Because They Cannot Portray Identity Groups},
  author = {Wang, Angelina and Morgenstern, Jamie and Dickerson, John P.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01908},
  eprint = {2402.01908},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01908},
  urldate = {2024-04-22},
  abstract = {Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains -- including as replacements for human participants in computational social science, user testing, annotation tasks, and more. Traditionally, in all of these settings survey distributors are careful to find representative samples of the human population to ensure the validity of their results and understand potential demographic differences. This means in order to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (i.e., relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are doomed to both misportray and flatten the representations of demographic groups, then empirically show this to be true on 4 LLMs through a series of human studies with 3200 participants across 16 demographic identities. We also discuss a third consideration about how identity prompts can essentialize identities. Throughout, we connect each of these limitations to a pernicious history that shows why each is harmful for marginalized demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the goal is to supplement rather than replace (e.g., pilot studies), we provide empirically-better inference-time techniques to reduce, but not remove, these harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/home/max/Zotero/storage/3GFXPXTR/Wang et al. - 2024 - Large language models cannot replace human partici.pdf;/home/max/Zotero/storage/QJE483U7/2402.html}
}

@misc{wang_zero-shot_2024,
  title = {Zero-Shot {{Generative Large Language Models}} for {{Systematic Review Screening Automation}}},
  author = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06320},
  eprint = {2401.06320},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-25},
  abstract = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/max/Zotero/storage/ANLFW2KY/Wang et al. - 2024 - Zero-shot Generative Large Language Models for Sys.pdf;/home/max/Zotero/storage/7TIF8XZN/2401.html}
}

@misc{wu_reasoning_2023,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02477},
  eprint = {2307.02477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.02477},
  urldate = {2023-08-01},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/E4GAX8RL/Wu et al. - 2023 - Reasoning or Reciting Exploring the Capabilities .pdf;/home/max/Zotero/storage/RFRGAXKH/2307.html}
}

@misc{xia_llmscreen_2024,
  title = {{{LLMscreen}}: {{A Python Package}} for {{Systematic Review Screening}} of {{Scientific Texts Using Prompt Engineering}}},
  shorttitle = {{{LLMscreen}}},
  author = {Xia, Ziqian and Ye, Jinquan and Hu, Bo and Qiang, Qiqi and Debnath, Ramit},
  year = {2024},
  month = sep,
  publisher = {Research Square},
  issn = {2693-5015},
  doi = {10.21203/rs.3.rs-5063165/v1},
  urldate = {2024-10-29},
  abstract = {Systematic reviews represent a cornerstone of evidence-based research, yet the process is labor-intensive and time-consuming, often requiring substantial human resources. The advent of Large Language Models (LLMs) offers a novel approach to streamlining systematic reviews, particularly in the title and abstract screening phase. This study introduces a new Python package built on LLMs to accelerate this process, evaluating its performance across three datasets using distinct prompt strategies: single-prompt, k-value setting, and zero-shot. The k-value setting approach emerged as the most effective, achieving a precision of 0.649 and reducing the average error rate to 0.4\%, significantly lower than the 10.76\% error rate typically observed among human reviewers. Moreover, this approach enabled the screening of 3,000 papers in under 8 minutes, at a cost of only \$0.30\&amp;mdash;an over 250-fold improvement in time and 2,000-fold cost efficiency compared to traditional methods. These findings underscore the potential of LLMs to enhance the efficiency and accuracy of systematic reviews, though further research is needed to address challenges related to dataset variability and model transparency. Expanding the application of LLMs to other stages of systematic reviews, such as data extraction and synthesis, could further streamline the review process, making it more comprehensive and less burdensome for researchers.},
  archiveprefix = {Research Square},
  file = {/home/max/Zotero/storage/FRT8TRBQ/Xia et al. - 2024 - LLMscreen A Python Package for Systematic Review .pdf}
}

@misc{yadlowsky_pretraining_2023,
  title = {Pretraining {{Data Mixtures Enable Narrow Model Selection Capabilities}} in {{Transformer Models}}},
  author = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  year = {2023},
  month = nov,
  number = {arXiv:2311.00871},
  eprint = {2311.00871},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-15},
  abstract = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/I2FKY9ST/Yadlowsky et al. - 2023 - Pretraining Data Mixtures Enable Narrow Model Sele.pdf;/home/max/Zotero/storage/VSTW3DJ6/2311.html}
}

@misc{yang_improving_2023,
  title = {Improving {{Probability-based Prompt Selection Through Unified Evaluation}} and {{Analysis}}},
  author = {Yang, Sohee and Kim, Jonghyeon and Jang, Joel and Ye, Seonghyeon and Lee, Hyunji and Seo, Minjoon},
  year = {2023},
  month = may,
  number = {arXiv:2305.14877},
  eprint = {2305.14877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14877},
  urldate = {2024-02-29},
  abstract = {Large Language Models (LLMs) have demonstrated great capabilities in solving a wide range of tasks in a resource-efficient manner through prompting, which does not require task-specific training, but suffers from performance fluctuation when there are multiple prompt candidates. Previous works have introduced gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but fail to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common NLP tasks. We find that all existing methods can be unified into some variant of the method that maximizes the mutual information between the input and the corresponding model output (denoted as MI). Using the finding, we develop several variants of MI and increases the effectiveness of the best prompt selection method from 87.79\% to 94.98\%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prompt. Furthermore, we propose a novel calibration method called Calibration by Marginalization (CBM) that is orthogonal to existing methods and helps increase the prompt selection effectiveness of the best method by 99.44\%. The code and datasets used in our work will be released at https://github.com/soheeyang/unified-prompt-selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/RX8QULHH/Yang et al. - 2023 - Improving Probability-based Prompt Selection Throu.pdf;/home/max/Zotero/storage/RHFIJX7U/2305.html}
}

@article{ziems2023css,
  title = {Can Large Language Models Transform Computational Social Science?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  year = {2023},
  month = apr,
  journal = {arXiv submission 4840038}
}
