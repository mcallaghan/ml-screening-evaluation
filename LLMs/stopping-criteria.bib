@book{baron_perspectives_2016,
  title = {Perspectives on {{Predictive Coding}}: {{And Other Advanced Search Methods}} for the {{Legal Practitioner}}},
  shorttitle = {Perspectives on {{Predictive Coding}}},
  author = {Baron, Jason R. and Losey, Ralph C. and Berman, Michael D.},
  year = {2016},
  publisher = {American Bar Association, Section of Litigation},
  abstract = {Reliance on manual and keyword search methods alone is increasingly seen as inadequate for searching large volumes of information. There are concerns about the accuracy and efficiency of these methods, especially as compared with more advanced search techniques. This book provides a set of perspectives on predictive coding and other advanced search techniques, as they are used today by lawyers in pursuit of e-discovery, in investigations, and in other legal contexts, such as information governance.There is something in this book for everyone--novices, seasoned e-discovery practitioners, litigators, business lawyers, and technologists. It is meant to appeal both to practitioners who are seeking basic knowledge of what predictive coding and other advanced search methods are all about, as well as to those members of the legal community who are "inside the bubble" of e-discovery already and wish to gain further insight into the latest thinking on advanced search techniques from leading lawyers, judges, and information scientists. The book may also be read by lawyers who do not consider themselves litigators or e-discovery practitioners, but who wish to apply a knowledge of smart analytics in other legal contexts.},
  googlebooks = {TdJ2AQAACAAJ},
  isbn = {978-1-63425-658-2},
  langid = {english},
  keywords = {Law / Depositions,Law / Evidence,Law / Litigation,Law / Research}
}

@article{boetje_safe_2024,
  title = {The {{SAFE}} Procedure: A Practical Stopping Heuristic for Active Learning-Based Screening in Systematic Reviews and Meta-Analyses},
  shorttitle = {The {{SAFE}} Procedure},
  author = {Boetje, Josien and {van de Schoot}, Rens},
  year = {2024},
  month = mar,
  journal = {Systematic Reviews},
  volume = {13},
  number = {1},
  pages = {81},
  issn = {2046-4053},
  doi = {10.1186/s13643-024-02502-7},
  urldate = {2024-04-23},
  abstract = {Active learning has become an increasingly popular method for screening large amounts of data in systematic reviews and meta-analyses. The active learning process continually improves its predictions on the remaining unlabeled records, with the goal of identifying all relevant records as early as possible. However, determining the optimal point at which to stop the active learning process is a challenge. The cost of additional labeling of records by the reviewer must be balanced against the cost of erroneous exclusions. This paper introduces the SAFE procedure, a practical and conservative set of stopping heuristics that offers a clear guideline for determining when to end the active learning process in screening software like ASReview. The eclectic mix of stopping heuristics helps to minimize the risk of missing relevant papers in the screening process. The proposed stopping heuristic balances the costs of continued screening with the risk of missing relevant records, providing a practical solution for reviewers to make informed decisions on when to stop screening. Although active learning can significantly enhance the quality and efficiency of screening, this method may be more applicable to certain types of datasets and problems. Ultimately, the decision to stop the active learning process depends on careful consideration of the trade-off between the costs of additional record labeling against the potential errors of the current model for the specific dataset and context.},
  langid = {english},
  keywords = {Active learning,Machine learning,Meta-analysis,Methodology,Screening prioritization,Stopping heuristic,Stopping rule,Systematic review},
  file = {/home/max/Zotero/storage/Q7NLVLCN/Boetje and van de Schoot - 2024 - The SAFE procedure a practical stopping heuristic.pdf}
}

@article{bornmann_growth_2015,
  title = {Growth Rates of Modern Science: {{A}} Bibliometric Analysis Based on the Number of Publications and Cited References},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Mutz, R{\"u}diger},
  year = {2015},
  journal = {Journal of the Association for Information Science and Technology},
  volume = {66},
  number = {11},
  pages = {2215--2222},
  issn = {2330-1643},
  doi = {10.1002/asi.23329},
  urldate = {2024-01-15},
  abstract = {Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique---segmented regression analysis---which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid-1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1\% up to the middle of the 18th century, to 2 to 3\% up to the period between the two world wars, and 8 to 9\% to 2010.},
  copyright = {{\copyright} 2015 ASIS\&T},
  langid = {english},
  keywords = {bibliometrics},
  file = {/home/max/Zotero/storage/HQRHL5ZU/Bornmann and Mutz - 2015 - Growth rates of modern science A bibliometric ana.pdf;/home/max/Zotero/storage/CEFG59QG/asi.html}
}

@misc{bubeck_sparks_2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2024-01-15},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/U4SS6FL9/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf;/home/max/Zotero/storage/IMSPX4E2/2303.html}
}

@article{callaghan_statistical_2020,
  title = {Statistical {{Stopping Criteria}} for {{Automated Screening}} in {{Systematic Reviews}}},
  author = {Callaghan, Max and {M{\"u}ller-Hansen}, Finn},
  year = {2020},
  month = sep,
  journal = {Systematic Reviews},
  doi = {10.21203/rs.2.18218/v2},
  urldate = {2020-11-25},
  abstract = {Active learning for systematic review screening promises to reduce the human effort required to identify relevant documents for a systematic review.~Machines and humans~ work together, with humans providing training data, and the machine optimising the documents that the humans screen. This enabl...},
  langid = {english},
  file = {/home/max/Zotero/storage/XDR25KQ8/2020 - Statistical Stopping Criteria for Automated Screen.pdf}
}

@article{castaldi_copd_2010,
  title = {The {{COPD}} Genetic Association Compendium: A Comprehensive Online Database of {{COPD}} Genetic Associations},
  shorttitle = {The {{COPD}} Genetic Association Compendium},
  author = {Castaldi, Peter J. and Cho, Michael H. and Cohn, Matthew and Langerman, Fawn and Moran, Sienna and Tarragona, Nestor and Moukhachen, Hala and Venugopal, Radhika and Hasimja, Delvina and Kao, Esther and Wallace, Byron and Hersh, Craig P. and Bagade, Sachin and Bertram, Lars and Silverman, Edwin K. and Trikalinos, Thomas A.},
  year = {2010},
  month = feb,
  journal = {Human Molecular Genetics},
  volume = {19},
  number = {3},
  pages = {526--534},
  issn = {0964-6906},
  doi = {10.1093/hmg/ddp519},
  urldate = {2023-09-08},
  abstract = {Chronic obstructive pulmonary disease (COPD) is a major cause of morbidity and mortality worldwide. COPD is thought to arise from the interaction of environmental exposures and genetic susceptibility, and major research efforts are underway to identify genetic determinants of COPD susceptibility. With the exception of SERPINA1, genetic associations with COPD identified by candidate gene studies have been inconsistently replicated, and this literature is difficult to interpret. We conducted a systematic review and meta-analysis of all population-based, case--control candidate gene COPD studies indexed in PubMed before 16 July 2008. We stored our findings in an online database, which serves as an up-to-date compendium of COPD genetic associations and cumulative meta-analysis estimates. On the basis of our systematic review, the vast majority of COPD candidate gene era studies are underpowered to detect genetic effect odds ratios of 1.2--1.5. We identified 27 genetic variants with adequate data for quantitative meta-analysis. Of these variants, four were significantly associated with COPD susceptibility in random effects meta-analysis, the GSTM1 null variant (OR 1.45, CI 1.09--1.92), rs1800470 in TGFB1 (0.73, CI 0.64--0.83), rs1800629 in TNF (OR 1.19, CI 1.01--1.40) and rs1799896 in SOD3 (OR 1.97, CI 1.24--3.13). In summary, most COPD candidate gene era studies are underpowered to detect moderate-sized genetic effects. Quantitative meta-analysis identified four variants in GSTM1, TGFB1, TNF and SOD3 that show statistically significant evidence of association with COPD susceptibility.},
  file = {/home/max/Zotero/storage/UF2FGKC2/Castaldi et al. - 2010 - The COPD genetic association compendium a compreh.pdf;/home/max/Zotero/storage/NXL7C26L/600434.html}
}

@article{chang_libsvm_2011,
  title = {{{LIBSVM}}: {{A}} Library for Support Vector Machines},
  shorttitle = {{{LIBSVM}}},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  year = {2011},
  month = may,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {2},
  number = {3},
  pages = {27:1--27:27},
  issn = {2157-6904},
  doi = {10.1145/1961189.1961199},
  urldate = {2020-10-27},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
  keywords = {Classification LIBSVM optimization regression support vector machines SVM}
}

@article{chappell_machine_2023,
  title = {Machine Learning for Accelerating Screening in Evidence Reviews},
  author = {Chappell, Mary and Edwards, Mary and Watkins, Deborah and Marshall, Christopher and Graziadio, Sara},
  year = {2023},
  journal = {Cochrane Evidence Synthesis and Methods},
  volume = {1},
  number = {5},
  pages = {e12021},
  issn = {2832-9023},
  doi = {10.1002/cesm.12021},
  urldate = {2024-01-15},
  abstract = {Evidence reviews are important for informing decision-making and primary research, but they can be time-consuming and costly. With the advent of artificial intelligence, including machine learning, there is an opportunity to accelerate the review process at many stages, with study screening identified as a prime candidate for assistance. Despite the availability of a large number of tools promising to assist with study screening, these are not consistently used in practice and there is skepticism about their application. Single-arm evaluations suggest the potential for tools to reduce screening burden. However, their integration into practice may need further investigation through evaluations of outcomes such as overall resource use and impact on review findings and recommendations. Because the literature lacks comparative studies, it is not currently possible to determine their relative accuracy. In this commentary, we outline the published research and discuss options for incorporating tools into the review workflow, considering the needs and requirements of different types of review.},
  copyright = {{\copyright} 2023 The Authors. Cochrane Evidence Synthesis and Methods published by John Wiley \& Sons Ltd on behalf of The Cochrane Collaboration.},
  langid = {english},
  keywords = {machine learning,rapid review,record screening,systematic review},
  file = {/home/max/Zotero/storage/HJE9IKUL/Chappell et al. - 2023 - Machine learning for accelerating screening in evi.pdf;/home/max/Zotero/storage/WN6DLHKM/cesm.html}
}

@article{cohen_reducing_2006,
  title = {Reducing Workload in Systematic Review Preparation Using Automated Citation Classification},
  author = {Cohen, Aaron M. and Hersh, William R. and Peterson, Kim and Yen, Po-Yin},
  year = {2006},
  journal = {Journal of the American Medical Informatics Association},
  volume = {13},
  number = {2},
  pages = {206--219},
  publisher = {BMJ Group BMA House, Tavistock Square, London, WC1H 9JR}
}

@inproceedings{cormack_engineering_2016,
  title = {Engineering {{Quality}} and {{Reliability}} in {{Technology-Assisted Review}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Cormack, Gordon V. and Grossman, Maura R.},
  year = {2016},
  month = jul,
  series = {{{SIGIR}} '16},
  pages = {75--84},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2911451.2911510},
  urldate = {2023-10-13},
  abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
  isbn = {978-1-4503-4069-4},
  keywords = {continuous active learning,e-discovery,electronic discovery,predictive coding,quality,relevance feedback,reliability,systematic review,technology-assisted review,test collections},
  file = {/home/max/Zotero/storage/HTCWCEVU/Cormack and Grossman - 2016 - Engineering Quality and Reliability in Technology-.pdf}
}

@inproceedings{cormack_evaluation_2014,
  title = {Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval},
  author = {Cormack, Gordon V. and Grossman, Maura R.},
  year = {2014},
  month = jul,
  series = {{{SIGIR}} '14},
  pages = {153--162},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2600428.2609601},
  urldate = {2024-01-15},
  abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P{$<$}0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P{$<$}0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
  isbn = {978-1-4503-2257-7},
  keywords = {e-discovery,electronic discovery,predictive coding,technology-assisted review},
  file = {/home/max/Zotero/storage/9CG2SW75/Cormack and Grossman - 2014 - Evaluation of machine-learning protocols for techn.pdf}
}

@misc{de_bruin_synergy_2023,
  title = {{{SYNERGY}} - {{Open}} Machine Learning Dataset on Study Selection in Systematic Reviews},
  author = {De Bruin, Jonathan and Ma, Yongchao and Ferdinands, Gerbrich and Teijema, Jelle and {Van de Schoot}, Rens},
  year = {2023},
  month = apr,
  publisher = {DataverseNL},
  doi = {10.34894/HE6NAQ},
  urldate = {2024-09-05},
  abstract = {SYNERGY is a free and open dataset on study selection in systematic reviews, comprising 169,288 academic works from 26 systematic reviews. Only 2,834 (1.67\%) of the academic works in the binary classified dataset are included in the systematic reviews. This makes the SYNERGY dataset a unique dataset for the development of information retrieval algorithms, especially for sparse labels. Due to the many available variables available per record (i.e. titles, abstracts, authors, references, topics), this dataset is useful for researchers in NLP, machine learning, network analysis, and more. In total, the dataset contains 82,668,134 trainable data points. The easiest way to get the SYNERGY dataset is via the synergy-dataset Python package. See https://github.com/asreview/synergy-dataset for all information.},
  copyright = {http://creativecommons.org/publicdomain/zero/1.0},
  langid = {english},
  keywords = {Computer and Information Science,Engineering,Health and Life Sciences,Medicine,Social Sciences},
  file = {/home/max/Zotero/storage/X3I8UWBY/dataset.html}
}

@misc{devlin_bert_2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-04-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/WUABD45Z/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/max/Zotero/storage/KZRSWKJC/1810.html}
}

@article{donnelly_four_2018,
  title = {Four Principles to Make Evidence Synthesis More Useful for Policy},
  author = {Donnelly, Christl A. and Boyd, Ian and Campbell, Philip and Craig, Claire and Vallance, Patrick and Walport, Mark and Whitty, Christopher J. M. and Woods, Emma and Wormald, Chris},
  year = {2018},
  month = jun,
  journal = {Nature},
  volume = {558},
  number = {7710},
  pages = {361--364},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-018-05414-4},
  urldate = {2024-01-15},
  abstract = {Reward the creation of analyses for policymakers that are inclusive, rigorous, transparent and accessible.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Funding,Policy,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Comment\\
Subject\_term: Policy, Research management, Funding},
  file = {/home/max/Zotero/storage/8BB8Z8ET/Donnelly et al. - 2018 - Four principles to make evidence synthesis more us.pdf;/home/max/Zotero/storage/8TNHDM6Y/d41586-018-05414-4.html}
}

@article{fog_calculation_2008,
  title = {Calculation {{Methods}} for {{Wallenius}}' {{Noncentral Hypergeometric Distribution}}},
  author = {Fog, Agner},
  year = {2008},
  month = feb,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {37},
  number = {2},
  pages = {258--273},
  issn = {0361-0918, 1532-4141},
  doi = {10.1080/03610910701790269},
  urldate = {2023-08-09},
  langid = {english},
  file = {/home/max/Zotero/storage/PMPEA4D4/Fog - 2008 - Calculation Methods for Wallenius' Noncentral Hype.pdf}
}

@article{gough_clarifying_2019,
  title = {Clarifying Differences between Reviews within Evidence Ecosystems},
  author = {Gough, David and Thomas, James and Oliver, Sandy},
  year = {2019},
  month = jul,
  journal = {Systematic Reviews},
  volume = {8},
  number = {1},
  pages = {170},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1089-2},
  urldate = {2024-03-21},
  abstract = {This paper builds on a 2012 paper by the same authors which argued that the types and brands of systematic review do not sufficiently differentiate between the many dimensions of different review questions and review methods (Gough et al., Syst Rev 1:28, 2012). The current paper extends this argument by considering the dynamic contexts, or `evidence ecosystems', within which reviews are undertaken; the fact that these ecosystems are constantly changing; and the relevance of this broader context for understanding `dimensions of difference' in the unfolding development and refinement of review methods.},
  file = {/home/max/Zotero/storage/Y7MMLJ46/Gough et al. - 2019 - Clarifying differences between reviews within evid.pdf;/home/max/Zotero/storage/B8VJQN9E/s13643-019-1089-2.html}
}

@article{hamel_guidance_2021,
  title = {Guidance for Using Artificial Intelligence for Title and Abstract Screening While Conducting Knowledge Syntheses},
  author = {Hamel, Candyce and Hersi, Mona and Kelly, Shannon E. and Tricco, Andrea C. and Straus, Sharon and Wells, George and Pham, Ba' and Hutton, Brian},
  year = {2021},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {21},
  number = {1},
  pages = {285},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01451-2},
  urldate = {2023-10-12},
  abstract = {Systematic reviews are the cornerstone of evidence-based medicine. However, systematic reviews are time consuming and there is growing demand to produce evidence more quickly, while maintaining robust methods. In recent years, artificial intelligence and active-machine learning (AML) have been implemented into several SR software applications. As some of the barriers to adoption of new technologies are the challenges in set-up and how best to use these technologies, we have provided different situations and considerations for knowledge synthesis teams to consider when using artificial intelligence and AML for title and abstract screening.},
  keywords = {Active machine-learning,Artificial intelligence,Best practice guidance,Knowledge Synthesis,Prioritization,Title and abstract screening},
  file = {/home/max/Zotero/storage/ZHU2YLYI/Hamel et al. - 2021 - Guidance for using artificial intelligence for tit.pdf;/home/max/Zotero/storage/4DRLW9NC/s12874-021-01451-2.html}
}

@book{higgins_cochrane_2019,
  title = {Cochrane {{Handbook}} for {{Systematic Reviews}} of {{Interventions}}},
  editor = {Higgins, {\relax JPT} and Thomas, J and Chandler, J and Cumpston, M and Li, T and Page, {\relax MJ} and Welch, {\relax VA}},
  year = {2019},
  edition = {2},
  publisher = {John Wiley \& Sons},
  address = {Chichester (UK)}
}

@article{howard_swift-active_2020,
  title = {{{SWIFT-Active Screener}}: {{Accelerated}} Document Screening through Active Learning and Integrated Recall Estimation},
  shorttitle = {{{SWIFT-Active Screener}}},
  author = {Howard, Brian E. and Phillips, Jason and Tandon, Arpit and Maharana, Adyasha and Elmore, Rebecca and Mav, Deepak and Sedykh, Alex and Thayer, Kristina and Merrick, B. Alex and Walker, Vickie and Rooney, Andrew and Shah, Ruchir R.},
  year = {2020},
  month = may,
  journal = {Environment International},
  volume = {138},
  pages = {105623},
  issn = {0160-4120},
  doi = {10.1016/j.envint.2020.105623},
  urldate = {2023-10-13},
  abstract = {Background In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate articles is relevant to the research question under consideration. A typical review may require screening thousands or tens of thousands of articles in and can utilize hundreds of person-hours of labor. Methods Here we introduce SWIFT-Active Screener, a web-based, collaborative systematic review software application, designed to reduce the overall screening burden required during this resource-intensive phase of the review process. To prioritize articles for review, SWIFT-Active Screener uses active learning, a type of machine learning that incorporates user feedback during screening. Meanwhile, a negative binomial model is employed to estimate the number of relevant articles remaining in the unscreened document list. Using a simulation involving 26 diverse systematic review datasets that were previously screened by reviewers, we evaluated both the document prioritization and recall estimation methods. Results On average, 95\% of the relevant articles were identified after screening only 40\% of the total reference list. In the 5 document sets with 5,000 or more references, 95\% recall was achieved after screening only 34\% of the available references, on average. Furthermore, the recall estimator we have proposed provides a useful, conservative estimate of the percentage of relevant documents identified during the screening process. Conclusion SWIFT-Active Screener can result in significant time savings compared to traditional screening and the savings are increased for larger project sizes. Moreover, the integration of explicit recall estimation during screening solves an important challenge faced by all machine learning systems for document screening: when to stop screening a prioritized reference list. The software is currently available in the form of a multi-user, collaborative, online web application.},
  keywords = {Active learning,Document screening,Evidence mapping,Machine learning,Recall estimation,Systematic review},
  file = {/home/max/Zotero/storage/LF7JVQRH/Howard et al. - 2020 - SWIFT-Active Screener Accelerated document screen.pdf;/home/max/Zotero/storage/KFV4UV44/S0160412019314023.html}
}

@article{jonnalagadda_new_2013,
  title = {A New Iterative Method to Reduce Workload in Systematic Review Process},
  author = {Jonnalagadda, Siddhartha and Petitti, Diana},
  year = {2013},
  month = jan,
  journal = {International Journal of Computational Biology and Drug Design},
  volume = {6},
  number = {1-2},
  pages = {5--17},
  publisher = {Inderscience Publishers},
  issn = {1756-0756},
  doi = {10.1504/IJCBDD.2013.052198},
  urldate = {2023-10-13},
  abstract = {High cost for systematic review of biomedical literature has generated interest in decreasing overall workload. This can be done by applying natural language processing techniques to `automate' the classification of publications that are potentially relevant for a given question. Existing solutions need training using a specific supervised machine-learning algorithm and feature-extraction system separately for each systematic review. We propose a system that only uses the input and feedback of human reviewers during the course of review. As the reviewers classify articles, the query is modified using a simple relevance feedback algorithm, and the semantically closest document to the query is presented. An evaluation of our approach was performed using a set of 15 published drug systematic reviews. The number of articles that needed to be reviewed was substantially reduced (ranging from 6\% to 30\% for a 95\% recall).},
  keywords = {distributional semantics,information retrieval,machine learning,systematic review},
  file = {/home/max/Zotero/storage/VKKF7S8D/Jonnalagadda and Petitti - 2013 - A new iterative method to reduce workload in syste.pdf}
}

@article{kugley_searching_2017,
  title = {Searching for Studies: A Guide to Information Retrieval for {{Campbell}} Systematic Reviews},
  shorttitle = {Searching for Studies},
  author = {Kugley, Shannon and Wade, Anne and Thomas, James and Mahood, Quenby and J{\o}rgensen, Anne-Marie Klint and Hammerstr{\o}m, Karianne and Sathe, Nila},
  year = {2017},
  journal = {Campbell Systematic Reviews},
  volume = {13},
  number = {1},
  pages = {1--73},
  issn = {1891-1803},
  doi = {10.4073/cmg.2016.1},
  urldate = {2024-01-17},
  copyright = {{\copyright} 2017 Kugley et al.},
  langid = {english},
  file = {/home/max/Zotero/storage/5TQQ9LC8/Kugley et al. - 2017 - Searching for studies a guide to information retr.pdf;/home/max/Zotero/storage/9LFGPAKK/cmg.2016.html}
}

@inbook{lefebvre_chapter_2019,
  title = {Chapter 4: {{Searching}} for and Selecting Studies},
  shorttitle = {Chapter 4},
  booktitle = {Cochrane {{Handbook}} for {{Systematic Reviews}} of {{Interventions}}},
  author = {Lefebvre, C and Glanville, J and Briscoe, S and {A Littlewood} and Marshall, Christopher and Metzendorf, M-I and {Noel-Storr}, A and Rader, T and Shokraneh, Farhad and Thomas, James and Wieland, {\relax LS}},
  year = {2019},
  edition = {Version 6 (updated October 2019)},
  urldate = {2024-01-15},
  collaborator = {Higgins, {\relax JPT} and Thomas, J and Chandler, J and Cumpston, M and Li, T and Page, {\relax MJ} and Welch, {\relax VA}},
  langid = {english}
}

@inbook{lefebvre_chapter_2023,
  title = {Chapter 4: {{Searching}} for and Selecting Studies},
  shorttitle = {Chapter 4},
  booktitle = {Cochrane {{Handbook}} for {{Systematic Reviews}} of {{Interventions}}},
  author = {Lefebvre, C and Glanville, J and Briscoe, S and Featherstone, R and Metzendorf, M-I and {Noel-Storr}, A and Paynter, R and Rader, T and Thomas, J and Wieland, {\relax LS}},
  year = {2023},
  edition = {Version 6.4 (updated October 2023)},
  urldate = {2024-01-15},
  collaborator = {Higgins, {\relax JPT} and Thomas, J and Chandler, J and Cumpston, M and Li, T and Page, {\relax MJ} and Welch, {\relax VA}},
  langid = {english},
  file = {/home/max/Zotero/storage/NJ7DCGS8/chapter-04.html}
}

@inproceedings{lewis_certifying_2021,
  title = {Certifying {{One-Phase Technology-Assisted Reviews}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Lewis, David D. and Yang, Eugene and Frieder, Ophir},
  year = {2021},
  month = oct,
  eprint = {2108.12746},
  primaryclass = {cs},
  pages = {893--902},
  doi = {10.1145/3459637.3482415},
  urldate = {2024-01-15},
  abstract = {Technology-assisted review (TAR) workflows based on iterative active learning are widely used in document review applications. Most stopping rules for one-phase TAR workflows lack valid statistical guarantees, which has discouraged their use in some legal contexts. Drawing on the theory of quantile estimation, we provide the first broadly applicable and statistically valid sample-based stopping rules for one-phase TAR. We further show theoretically and empirically that overshooting a recall target, which has been treated as innocuous or desirable in past evaluations of stopping rules, is a major source of excess cost in one-phase TAR workflows. Counterintuitively, incurring a larger sampling cost to reduce excess recall leads to lower total cost in almost all scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/RPMY9LSL/Lewis et al. - 2021 - Certifying One-Phase Technology-Assisted Reviews.pdf;/home/max/Zotero/storage/EU5IJBSF/2108.html}
}

@inproceedings{lewis_confidence_2023,
  title = {Confidence {{Sequences}} for {{Evaluating One-Phase Technology-Assisted Review}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Lewis, David D. and Gray, Lenora and Noel, Mark},
  year = {2023},
  month = sep,
  series = {{{ICAIL}} '23},
  pages = {131--140},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3594536.3595167},
  urldate = {2023-09-12},
  abstract = {Technology-assisted review (TAR) workflows are central to electronic discovery (eDiscovery). Researchers have proposed many methods for evaluating TAR workflows, but this research has had little impact on eDiscovery practice. We examine the operational constraints faced by eDiscovery reviewers and managers, and show how past evaluation proposals are inconsistent with their needs. We then present a new evaluation approach for one-phase TAR workflows based on confidence sequences. Our approach provides a review manager with complete control over the design and duration of the TAR workflow, as well as the amount and timing of review of evaluation documents. Evaluation documents can be reused for supervised learning while preserving valid frequentist confidence intervals on recall at all points during review. The method is expensive in terms of sample size but plausible for large scale reviews, and has many opportunities for improvement.},
  isbn = {9798400701979},
  keywords = {conformal prediction,e-disclosure,e-discovery,eDisclosure,effectiveness metrics,martingales,safe anytime-valid inference,SAVI},
  file = {/home/max/Zotero/storage/EWUZNP79/Lewis et al. - 2023 - Confidence Sequences for Evaluating One-Phase Tech.pdf}
}

@article{li_when_2020,
  title = {When to {{Stop Reviewing}} in {{Technology-Assisted Reviews}}: {{Sampling}} from an {{Adaptive Distribution}} to {{Estimate Residual Relevant Documents}}},
  shorttitle = {When to {{Stop Reviewing}} in {{Technology-Assisted Reviews}}},
  author = {Li, Dan and Kanoulas, Evangelos},
  year = {2020},
  month = sep,
  journal = {ACM Transactions on Information Systems},
  volume = {38},
  number = {4},
  pages = {41:1--41:36},
  issn = {1046-8188},
  doi = {10.1145/3411755},
  urldate = {2024-02-05},
  abstract = {Technology-Assisted Reviews (TAR) aim to expedite document reviewing (e.g., medical articles or legal documents) by iteratively incorporating machine learning algorithms and human feedback on document relevance. Continuous Active Learning (CAL) algorithms have demonstrated superior performance compared to other methods in efficiently identifying relevant documents. One of the key challenges for CAL algorithms is deciding when to stop displaying documents to reviewers. Existing work either lacks transparency---it provides an ad-hoc stopping point, without indicating how many relevant documents are still not found, or lacks efficiency by paying an extra cost to estimate the total number of relevant documents in the collection prior to the actual review. In this article, we handle the problem of deciding the stopping point of TAR under the continuous active learning framework by jointly training a ranking model to rank documents, and by conducting a ``greedy'' sampling to estimate the total number of relevant documents in the collection. We prove the unbiasedness of the proposed estimators under a with-replacement sampling design, while experimental results demonstrate that the proposed approach, similar to CAL, effectively retrieves relevant documents; but it also provides a transparent, accurate, and effective stopping point.},
  keywords = {active sampling,Total recall,unbiased estimator},
  file = {/home/max/Zotero/storage/98T5YI9R/Li and Kanoulas - 2020 - When to Stop Reviewing in Technology-Assisted Revi.pdf}
}

@article{loertscher_general_2017,
  title = {A General Non-Central Hypergeometric Distribution},
  author = {Loertscher, Simon and Muir, Ellen V. and Taylor, Peter G.},
  year = {2017},
  month = may,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {46},
  number = {9},
  pages = {4579--4598},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610926.2015.1088033},
  urldate = {2023-08-09},
  langid = {english},
  file = {/home/max/Zotero/storage/T5MU4M6J/Loertscher et al. - 2017 - A general non-central hypergeometric distribution.pdf}
}

@misc{luo_evaluating_2024,
  title = {Evaluating the {{Efficacy}} of {{Large Language Models}} for {{Systematic Review}} and {{Meta-Analysis Screening}}},
  author = {Luo, Ronald and Sastimoglu, Ziya and Faisal, Abu Ilius and Deen, M. Jamal},
  year = {2024},
  month = jun,
  pages = {2024.06.03.24308405},
  publisher = {medRxiv},
  doi = {10.1101/2024.06.03.24308405},
  urldate = {2024-11-04},
  abstract = {Background Systematic reviews and meta-analyses are essential for informed research and policymaking, yet they are typically resource-intensive and time-consuming. Recent advances in artificial intelligence and machine learning offer promising opportunities to streamline these processes. Objective To enhance the efficiency of systematic reviews, we explored the automation of various stages using GPT-3.5 Turbo. We assessed the model's efficacy and performance by comparing it against three expert-conducted reviews across a comprehensive dataset of 24,534 studies. Methods The model's performance was evaluated through a comparison with three expert reviews, utilizing a pseudo-K-folds permutation and a one-tailed ANOVA with an alpha level of 0.05 to ensure statistical validity. Key performance metrics such as accuracy, sensitivity, specificity, predictive values, F1-score, and the Matthews correlation coefficient were analyzed using two sets of prompts. Results Our approach significantly streamlined the systematic review process, which typically takes a year, reducing it to a few hours without sacrificing quality. In the initial screening phase, accuracy, specificity, and negative predictive values ranged between 80\% and 95\%. Sensitivity improved markedly during the second screening phase, demonstrating the model's robustness when provided with more extensive data. Conclusion While ongoing refinements are needed, this tool represents a significant advancement in research methodologies, potentially making systematic reviews more accessible to a wider range of researchers. Impact Statement Our manuscript presents a novel review screening protocol built using open-source frameworks, which significantly enhances the systematic review process in terms of efficiency and cost-effectiveness. Leveraging the capabilities of GPT and embedding models, our protocol demonstrates the potential to transform a traditionally time-consuming and expensive task into an accelerated and economical operation, all while maintaining high standards of accuracy and reliability. Key PointsGPT screening can streamline systematic reviews from a year-long, expensive process to just hours at minimal cost.Validated across different topics, the protocol exhibits high reliability and consistency in study inclusion.The AI-driven process reduces human bias, with prompt optimization considerably improving sensitivity.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@misc{luo_evaluating_2024-1,
  title = {Evaluating the {{Efficacy}} of {{Large Language Models}} for {{Systematic Review}} and {{Meta-Analysis Screening}}},
  author = {Luo, Ronald and Sastimoglu, Ziya and Faisal, Abu Ilius and Deen, M. Jamal},
  year = {2024},
  month = jun,
  pages = {2024.06.03.24308405},
  publisher = {medRxiv},
  doi = {10.1101/2024.06.03.24308405},
  urldate = {2024-11-04},
  abstract = {Background Systematic reviews and meta-analyses are essential for informed research and policymaking, yet they are typically resource-intensive and time-consuming. Recent advances in artificial intelligence and machine learning offer promising opportunities to streamline these processes. Objective To enhance the efficiency of systematic reviews, we explored the automation of various stages using GPT-3.5 Turbo. We assessed the model's efficacy and performance by comparing it against three expert-conducted reviews across a comprehensive dataset of 24,534 studies. Methods The model's performance was evaluated through a comparison with three expert reviews, utilizing a pseudo-K-folds permutation and a one-tailed ANOVA with an alpha level of 0.05 to ensure statistical validity. Key performance metrics such as accuracy, sensitivity, specificity, predictive values, F1-score, and the Matthews correlation coefficient were analyzed using two sets of prompts. Results Our approach significantly streamlined the systematic review process, which typically takes a year, reducing it to a few hours without sacrificing quality. In the initial screening phase, accuracy, specificity, and negative predictive values ranged between 80\% and 95\%. Sensitivity improved markedly during the second screening phase, demonstrating the model's robustness when provided with more extensive data. Conclusion While ongoing refinements are needed, this tool represents a significant advancement in research methodologies, potentially making systematic reviews more accessible to a wider range of researchers. Impact Statement Our manuscript presents a novel review screening protocol built using open-source frameworks, which significantly enhances the systematic review process in terms of efficiency and cost-effectiveness. Leveraging the capabilities of GPT and embedding models, our protocol demonstrates the potential to transform a traditionally time-consuming and expensive task into an accelerated and economical operation, all while maintaining high standards of accuracy and reliability. Key PointsGPT screening can streamline systematic reviews from a year-long, expensive process to just hours at minimal cost.Validated across different topics, the protocol exhibits high reliability and consistency in study inclusion.The AI-driven process reduces human bias, with prompt optimization considerably improving sensitivity.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@article{macdonald_searching_2024,
  title = {Searching for Studies: {{A}} Guide to Information Retrieval for {{Campbell}} Systematic Reviews},
  shorttitle = {Searching for Studies},
  author = {MacDonald, Heather and Comer, Cozette and Foster, Margaret and Labelle, Patrick R. and Marsalis, Scott and Nyhan, Kate and Premji, Zahra and Rogers, Morwenna and Splenda, Ryan and Stansfield, Claire and Young, Sarah},
  year = {2024},
  journal = {Campbell Systematic Reviews},
  volume = {20},
  number = {3},
  pages = {e1433},
  issn = {1891-1803},
  doi = {10.1002/cl2.1433},
  urldate = {2024-09-18},
  abstract = {This guide outlines general issues in searching for studies; describes the main sources of potential studies; and discusses how to plan the search process, design, and carry out search strategies, manage references found during the search process and document and report the search process.},
  copyright = {{\copyright} 2024 The Author(s). Campbell Systematic Reviews published by John Wiley \& Sons Ltd on behalf of The Campbell Collaboration.},
  langid = {english},
  keywords = {search,systematic reviews},
  file = {/home/max/Zotero/storage/BGBMUL6V/MacDonald et al. - 2024 - Searching for studies A guide to information retr.pdf;/home/max/Zotero/storage/GVHIXS3D/cl2.html}
}

@article{michelson_significant_2019,
  title = {The Significant Cost of Systematic Reviews and Meta-Analyses: {{A}} Call for Greater Involvement of Machine Learning to Assess the Promise of Clinical Trials},
  shorttitle = {The Significant Cost of Systematic Reviews and Meta-Analyses},
  author = {Michelson, Matthew and Reuter, Katja},
  year = {2019},
  month = dec,
  journal = {Contemporary Clinical Trials Communications},
  volume = {16},
  pages = {100443},
  issn = {2451-8654},
  doi = {10.1016/j.conctc.2019.100443},
  urldate = {2024-01-15},
  abstract = {Background More than 90\% of clinical-trial compounds fail to demonstrate sufficient efficacy and safety. To help alleviate this issue, systematic literature review and meta-analysis (SLR), which synthesize current evidence for a research question, can be applied to preclinical evidence to identify the most promising therapeutics. However, these methods remain time-consuming and labor-intensive. Here, we introduce an economic formula to estimate the expense of SLR for academic institutions and pharmaceutical companies. Methods We estimate the manual effort involved in SLR by quantifying the amount of labor required and the total associated labor cost. We begin with an empirical estimation and derive a formula that quantifies and describes the cost. Results The formula estimated that each SLR costs approximately \$141,194.80. We found that on average, the ten largest pharmaceutical companies publish 118.71 and the ten major academic institutions publish 132.16 SLRs per year. On average, the total cost of all SLRs per year to each academic institution amounts to \$18,660,304.77 and for each pharmaceutical company is \$16,761,234.71. Discussion It appears that SLR is an important, but costly mechanisms to assess the totality of evidence. Conclusions With the increase in the number of publications, the significant time and cost of SLR may pose a barrier to their consistent application to assess the promise of clinical trials thoroughly. We call on investigators and developers to develop automated solutions to help with the assessment of preclinical evidence particularly. The formula we introduce provides a cost baseline against which the efficiency of automation can be measured.},
  keywords = {Artificial intelligence,Automation,Clinical research,Clinical trial,Dollar cost,Labor costs,Machine learning,Meta-analysis,Systematic review},
  file = {/home/max/Zotero/storage/P3Z25IBQ/Michelson and Reuter - 2019 - The significant cost of systematic reviews and met.pdf;/home/max/Zotero/storage/BAHUE4ZP/S2451865419302054.html}
}

@article{molinari_sal_2024,
  title = {{{SAL$\tau$}}: Efficiently Stopping {{TAR}} by Improving Priors Estimates},
  shorttitle = {{{SAL$\tau$}}},
  author = {Molinari, Alessio and Esuli, Andrea},
  year = {2024},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {38},
  number = {2},
  pages = {535--568},
  issn = {1573-756X},
  doi = {10.1007/s10618-023-00961-5},
  urldate = {2024-03-21},
  abstract = {In high recall retrieval tasks, human experts review a large pool of documents with the goal of satisfying an information need. Documents are prioritized for review through an active learning policy, and the process is usually referred to as Technology-Assisted Review (TAR). TAR tasks also aim to stop the review process once the target recall is achieved to minimize the annotation cost. In this paper, we introduce a new stopping rule called SAL\$\$\_{\textbackslash}tau {\textasciicircum}R\$\$(SLD for Active Learning), a modified version of the Saerens--Latinne--Decaestecker algorithm (SLD) that has been adapted for use in active learning. Experiments show that our algorithm stops the review well ahead of the current state-of-the-art methods, while providing the same guarantees of achieving the target recall.},
  langid = {english},
  keywords = {Active learning,e-Discovery,Systematic review,TAR,Technology-assisted review},
  file = {/home/max/Zotero/storage/3PKZH84X/Molinari and Esuli - 2024 - SALτ efficiently stopping TAR by improving priors.pdf}
}

@misc{noauthor_how_nodate,
  title = {How to Stop Screening? {$\cdot$} Asreview/Asreview {$\cdot$} {{Discussion}} \#557},
  shorttitle = {How to Stop Screening?},
  journal = {GitHub},
  urldate = {2023-10-12},
  abstract = {This first post is continuously updated based on the discussions in this thread In the active learning cycle, the model incrementally improves its predictions on the remaining unlabeled records, bu...},
  howpublished = {https://github.com/asreview/asreview/discussions/557},
  langid = {english},
  file = {/home/max/Zotero/storage/W6B4X593/557.html}
}

@misc{noauthor_systematic_nodate,
  title = {The {{Systematic Review Toolbox}}},
  urldate = {2023-10-12},
  howpublished = {http://systematicreviewtools.com/software.php},
  file = {/home/max/Zotero/storage/Z8QA4GVZ/software.html}
}

@article{norman_measuring_2019,
  title = {Measuring the Impact of Screening Automation on Meta-Analyses of Diagnostic Test Accuracy},
  author = {Norman, Christopher R. and Leeflang, Mariska M. G. and Porcher, Rapha{\"e}l and N{\'e}v{\'e}ol, Aur{\'e}lie},
  year = {2019},
  month = oct,
  journal = {Systematic Reviews},
  volume = {8},
  number = {1},
  pages = {243},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1162-x},
  urldate = {2024-03-21},
  abstract = {The large and increasing number of new studies published each year is making literature identification in systematic reviews ever more time-consuming and costly. Technological assistance has been suggested as an alternative to the conventional, manual study identification to mitigate the cost, but previous literature has mainly evaluated methods in terms of recall (search sensitivity) and workload reduction. There is a need to also evaluate whether screening prioritization methods leads to the same results and conclusions as exhaustive manual screening. In this study, we examined the impact of one screening prioritization method based on active learning on sensitivity and specificity estimates in systematic reviews of diagnostic test accuracy.},
  keywords = {*Machine learning,*Systematic review as topic,Evidence based medicine,Natural language processing/*methods},
  file = {/home/max/Zotero/storage/GUNLWVIE/Norman et al. - 2019 - Measuring the impact of screening automation on me.pdf;/home/max/Zotero/storage/6XTQ5CI5/s13643-019-1162-x.html}
}

@article{oami_performance_2024,
  title = {Performance of a {{Large Language Model}} in {{Screening Citations}}},
  author = {Oami, Takehiko and Okada, Yohei and Nakada, Taka-aki},
  year = {2024},
  month = jul,
  journal = {JAMA Network Open},
  volume = {7},
  number = {7},
  pages = {e2420496},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2024.20496},
  urldate = {2024-11-04},
  abstract = {Large language models (LLMs) are promising as tools for citation screening in systematic reviews. However, their applicability has not yet been determined.To evaluate the accuracy and efficiency of an LLM in title and abstract literature screening.This prospective diagnostic study used the data from the title and abstract screening process for 5 clinical questions (CQs) in the development of the Japanese Clinical Practice Guidelines for Management of Sepsis and Septic Shock. The LLM decided to include or exclude citations based on the inclusion and exclusion criteria in terms of patient, population, problem; intervention; comparison; and study design of the selected CQ and was compared with the conventional method for title and abstract screening. This study was conducted from January 7 to 15, 2024.LLM (GPT-4 Turbo)--assisted citation screening or the conventional method.The sensitivity and specificity of the LLM-assisted screening process was calculated, and the full-text screening result using the conventional method was set as the reference standard in the primary analysis. Pooled sensitivity and specificity were also estimated, and screening times of the 2 methods were compared.In the conventional citation screening process, 8 of 5634 publications in CQ 1, 4 of 3418 in CQ 2, 4 of 1038 in CQ 3, 17 of 4326 in CQ 4, and 8 of 2253 in CQ 5 were selected. In the primary analysis of 5 CQs, LLM-assisted citation screening demonstrated an integrated sensitivity of 0.75 (95\% CI, 0.43 to 0.92) and specificity of 0.99 (95\% CI, 0.99 to 0.99). Post hoc modifications to the command prompt improved the integrated sensitivity to 0.91 (95\% CI, 0.77 to 0.97) without substantially compromising specificity (0.98 [95\% CI, 0.96 to 0.99]). Additionally, LLM-assisted screening was associated with reduced time for processing 100 studies (1.3 minutes vs 17.2 minutes for conventional screening methods; mean difference, -15.25 minutes [95\% CI, -17.70 to -12.79 minutes]).In this prospective diagnostic study investigating the performance of LLM-assisted citation screening, the model demonstrated acceptable sensitivity and reasonably high specificity with reduced processing time. This novel method could potentially enhance efficiency and reduce workload in systematic reviews.},
  file = {/home/max/Zotero/storage/T7UKCS89/Oami et al. - 2024 - Performance of a Large Language Model in Screening.pdf;/home/max/Zotero/storage/P6XV9LRP/2820861.html}
}

@article{omara-eves_using_2015,
  title = {Using Text Mining for Study Identification in Systematic Reviews: A Systematic Review of Current Approaches},
  shorttitle = {Using Text Mining for Study Identification in Systematic Reviews},
  author = {{O'Mara-Eves}, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
  year = {2015},
  month = jan,
  journal = {Systematic Reviews},
  volume = {4},
  number = {1},
  pages = {5},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-5},
  urldate = {2022-10-19},
  abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
  keywords = {Automation,Review efficiency,Screening,Study selection,Text mining},
  file = {/home/max/Zotero/storage/TI8FWMQ2/O’Mara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf;/home/max/Zotero/storage/4NW9SUFX/2046-4053-4-5.html}
}

@article{page_prisma_2021,
  title = {The {{PRISMA}} 2020 Statement: An Updated Guideline for Reporting Systematic Reviews},
  shorttitle = {The {{PRISMA}} 2020 Statement},
  author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hr{\'o}bjartsson, Asbj{\o}rn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and {Mayo-Wilson}, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
  year = {2021},
  month = mar,
  journal = {BMJ},
  volume = {372},
  pages = {n71},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.n71},
  urldate = {2024-01-17},
  abstract = {{$<$}p{$>$}The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {33782057},
  file = {/home/max/Zotero/storage/VP2CSH6I/Page et al. - 2021 - The PRISMA 2020 statement an updated guideline fo.pdf}
}

@article{petticrew_systematic_2001,
  title = {Systematic Reviews from Astronomy to Zoology: Myths and Misconceptions},
  shorttitle = {Systematic Reviews from Astronomy to Zoology},
  author = {Petticrew, Mark},
  year = {2001},
  month = jan,
  journal = {BMJ : British Medical Journal},
  volume = {322},
  number = {7278},
  pages = {98--101},
  issn = {0959-8138},
  urldate = {2023-10-06},
  pmcid = {PMC1119390},
  pmid = {11154628},
  file = {/home/max/Zotero/storage/UKWR2JD5/Petticrew - 2001 - Systematic reviews from astronomy to zoology myth.pdf}
}

@article{przybyla_prioritising_2018,
  title = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}: {{A}} User Study},
  shorttitle = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}},
  author = {Przyby{\l}a, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Le Pogam, Marie-Annick and McNaught, John and {von Elm}, Erik and Nolan, Kay and Ananiadou, Sophia},
  year = {2018},
  journal = {Research Synthesis Methods},
  volume = {9},
  number = {3},
  pages = {470--488},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1311},
  urldate = {2023-10-13},
  abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
  copyright = {{\copyright} 2018 The Authors. Research Synthesis Methods Published by John Wiley \& Sons Ltd.},
  langid = {english},
  file = {/home/max/Zotero/storage/GF75M8M4/Przybyła et al. - 2018 - Prioritising references for systematic reviews wit.pdf;/home/max/Zotero/storage/T6QEN85M/jrsm.html}
}

@article{rathbone_faster_2015,
  title = {Faster Title and Abstract Screening? {{Evaluating Abstrackr}}, a Semi-Automated Online Screening Program for Systematic Reviewers},
  shorttitle = {Faster Title and Abstract Screening?},
  author = {Rathbone, John and Hoffmann, Tammy and Glasziou, Paul},
  year = {2015},
  month = jun,
  journal = {Systematic Reviews},
  volume = {4},
  number = {1},
  pages = {80},
  issn = {2046-4053},
  doi = {10.1186/s13643-015-0067-6},
  urldate = {2024-05-08},
  abstract = {Citation screening is time consuming and inefficient. We sought to evaluate the performance of Abstrackr, a semi-automated online tool for predictive title and abstract screening.},
  keywords = {Eculizumab,False Negative Rate,Hemolytic Uremic Syndrome,Imbalanced Dataset,Recall Accuracy},
  file = {/home/max/Zotero/storage/YGU8D9QJ/Rathbone et al. - 2015 - Faster title and abstract screening Evaluating Ab.pdf;/home/max/Zotero/storage/4EQFM4PK/s13643-015-0067-6.html}
}

@incollection{robertson_probability_1997,
  title = {The Probability Ranking Principle in {{IR}}},
  booktitle = {Readings in Information Retrieval},
  author = {Robertson, S. E.},
  year = {1997},
  month = dec,
  pages = {281--286},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2024-02-05},
  isbn = {978-1-55860-454-4}
}

@article{saeidmehr_systematic_2024,
  title = {Systematic Review Using a Spiral Approach with Machine Learning},
  author = {Saeidmehr, Amirhossein and Steel, Piers~David~Gareth and Samavati, Faramarz~F.},
  year = {2024},
  month = jan,
  journal = {Systematic Reviews},
  volume = {13},
  number = {1},
  pages = {32},
  issn = {2046-4053},
  doi = {10.1186/s13643-023-02421-z},
  urldate = {2024-05-08},
  abstract = {With the accelerating growth of the academic corpus, doubling every 9 years, machine learning is a promising avenue to make systematic review manageable. Though several notable advancements have already been made, the incorporation of machine learning is less than optimal, still relying on a sequential, staged process designed to accommodate a purely human approach, exemplified by PRISMA. Here, we test a spiral, alternating or oscillating approach, where full-text screening is done intermittently with title/abstract screening, which we examine in three datasets by simulation under 360 conditions comprised of different algorithmic classifiers, feature extractions, prioritization rules, data types, and information provided (e.g., title/abstract, full-text included). Overwhelmingly, the results favored a spiral processing approach with logistic regression, TF-IDF for vectorization, and maximum probability for prioritization. Results demonstrate up to a 90\% improvement over traditional machine learning methodologies, especially for databases with fewer eligible articles. With these advancements, the screening component of most systematic reviews should remain functionally achievable for another one to two decades.},
  keywords = {Active learning,Machine learning,Systematic review,Technology-assisted review},
  file = {/home/max/Zotero/storage/NS9KKEPV/Saeidmehr et al. - 2024 - Systematic review using a spiral approach with mac.pdf;/home/max/Zotero/storage/PVVIY8HZ/s13643-023-02421-z.html}
}

@incollection{saldanha_modernizing_2023,
  title = {{Modernizing evidence synthesis for evidence-based medicine}},
  booktitle = {{Clinical Decision Support and beyond: Progress and Opportunities in Knowledge-Enhanced Health and Healthcare}},
  author = {Saldanha, Ian Jude and Adam, Gaelen P. and Schmid, Christopher H. and Trikalinos, Thomas A. and Konnyu, Kristin J.},
  year = {2023},
  month = jan,
  pages = {257--278},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-323-91200-6.00006-1},
  urldate = {2024-01-15},
  langid = {English (US)},
  file = {/home/max/Zotero/storage/9BLK8TZH/modernizing-evidence-synthesis-for-evidence-based-medicine.html}
}

@misc{scherbakov_emergence_2024,
  title = {The Emergence of {{Large Language Models}} ({{LLM}}) as a Tool in Literature Reviews: An {{LLM}} Automated Systematic Review},
  shorttitle = {The Emergence of {{Large Language Models}} ({{LLM}}) as a Tool in Literature Reviews},
  author = {Scherbakov, Dmitry and Hubig, Nina and Jansari, Vinita and Bakumenko, Alexander and Lenert, Leslie A.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.04600},
  eprint = {2409.04600},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.04600},
  urldate = {2024-11-04},
  abstract = {Objective: This study aims to summarize the usage of Large Language Models (LLMs) in the process of creating a scientific review. We look at the range of stages in a review that can be automated and assess the current state-of-the-art research projects in the field. Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model. ChatGPT was used to clean extracted data and generate code for figures in this manuscript, ChatGPT and Scite.ai were used in drafting all components of the manuscript, except the methods and discussion sections. Results: 3,788 articles were retrieved, and 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n=126, 73.2\%). A significant number of review automation projects were found, but only a limited number of papers (n=26, 15.1\%) were actual reviews that used LLM during their creation. Most citations focused on automation of a particular stage of review, such as Searching for publications (n=60, 34.9\%), and Data extraction (n=54, 31.4\%). When comparing pooled performance of GPT-based and BERT-based models, the former were better in data extraction with mean precision 83.0\% (SD=10.4), and recall 86.0\% (SD=9.8), while being slightly less accurate in title and abstract screening stage (Maccuracy=77.3\%, SD=13.0). Discussion/Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. The results looked promising, and we anticipate that LLMs will change in the near future the way the scientific reviews are conducted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries},
  file = {/home/max/Zotero/storage/KIBVBVMZ/Scherbakov et al. - 2024 - The emergence of Large Language Models (LLM) as a .pdf;/home/max/Zotero/storage/GKUNHNCC/2409.html}
}

@inproceedings{scholer_effect_2013,
  title = {The Effect of Threshold Priming and Need for Cognition on Relevance Calibration and Assessment},
  booktitle = {Proceedings of the 36th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Scholer, Falk and Kelly, Diane and Wu, Wan-Ching and Lee, Hanseul S. and Webber, William},
  year = {2013},
  month = jul,
  series = {{{SIGIR}} '13},
  pages = {623--632},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2484028.2484090},
  urldate = {2024-03-21},
  abstract = {Human assessments of document relevance are needed for the construction of test collections, for ad-hoc evaluation, and for training text classifiers. Showing documents to assessors in different orderings, however, may lead to different assessment outcomes. We examine the effect that {\textbackslash}defineterm\{threshold priming\}, seeing varying degrees of relevant documents, has on people's calibration of relevance. Participants judged the relevance of a prologue of documents containing highly relevant, moderately relevant, or non-relevant ocuments, followed by a common epilogue of documents of mixed relevance. We observe that participants exposed to only non-relevant documents in the prologue assigned significantly higher average relevance scores to prologue and epilogue documents than participants exposed to moderately or highly relevant documents in the prologue. We also examine how {\textbackslash}defineterm\{need for cognition\}, an individual difference measure of the extent to which a person enjoys engaging in effortful cognitive activity, impacts relevance assessments. High need for cognition participants had a significantly higher level of agreement with expert assessors than low need for cognition participants did. Our findings indicate that assessors should be exposed to documents from multiple relevance levels early in the judging process, in order to calibrate their relevance thresholds in a balanced way, and that individual difference measures might be a useful way to screen assessors.},
  isbn = {978-1-4503-2034-4},
  keywords = {assessors,calibration,evaluation,need for cognition,order effects,relevance assessments,relevance behavior,threshold priming}
}

@misc{sneyd_modelling_2019,
  title = {Modelling {{Stopping Criteria}} for {{Search Results}} Using {{Poisson Processes}}},
  author = {Sneyd, Alison and Stevenson, Mark},
  year = {2019},
  month = sep,
  number = {arXiv:1909.06239},
  eprint = {1909.06239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.06239},
  urldate = {2024-02-05},
  abstract = {Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for relevance by predicting when a suitable level of recall has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a Poisson process. This method allows a user to specify both a minimum desired level of recall to achieve and a desired probability of having achieved it. We evaluate our method on a public dataset and compare it with previous techniques for determining stopping criteria.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/max/Zotero/storage/UADWYZ6J/Sneyd and Stevenson - 2019 - Modelling Stopping Criteria for Search Results usi.pdf;/home/max/Zotero/storage/8A7H3SUI/1909.html}
}

@inproceedings{SneydS19,
  title = {Modelling Stopping Criteria for Search Results Using Poisson Processes},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {{EMNLP-IJCNLP}} 2019, Hong Kong, China, November 3-7, 2019},
  author = {Sneyd, Alison and Stevenson, Mark},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and 0001, Xiaojun Wan},
  year = {2019},
  pages = {3482--3487},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/D19-1351},
  citedby = {0},
  cites = {0},
  isbn = {978-1-950737-90-1}
}

@article{stansfield_applying_2022,
  title = {Applying Machine Classifiers to Update Searches: {{Analysis}} from Two Case Studies},
  shorttitle = {Applying Machine Classifiers to Update Searches},
  author = {Stansfield, Claire and Stokes, Gillian and Thomas, James},
  year = {2022},
  journal = {Research Synthesis Methods},
  volume = {13},
  number = {1},
  pages = {121--133},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1537},
  urldate = {2024-03-21},
  abstract = {Manual screening of citation records could be reduced by using machine classifiers to remove records of very low relevance. This seems particularly feasible for update searches, where a machine classifier can be trained from past screening decisions. However, feasibility is unclear for broad topics. We evaluate the performance and implementation of machine classifiers for update searches of public health research using two case studies. The first study evaluates the impact of using different sets of training data on classifier performance, comparing recall and screening reduction with a manual screening `gold standard'. The second study uses screening decisions from a review to train a classifier that is applied to rank the update search results. A stopping threshold was applied in the absence of a gold standard. Time spent screening titles and abstracts of different relevancy-ranked records was measured. Results: Study one: Classifier performance varies according to the training data used; all custom-built classifiers had a recall above 93\% at the same threshold, achieving screening reductions between 41\% and 74\%. Study two: applying a classifier provided a solution for tackling a large volume of search results from the update search, and screening volume was reduced by 61\%. A tentative estimate indicates over 25 h screening time was saved. In conclusion, custom-built machine classifiers are feasible for reducing screening workload from update searches across a range of public health interventions, with some limitation on recall. Key considerations include selecting a training dataset, agreeing stopping thresholds and processes to ensure smooth workflows.},
  copyright = {{\copyright} 2021 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {information retrieval,supervised machine learning,systematic reviews as topic,update search},
  file = {/home/max/Zotero/storage/6K54YS5Z/Stansfield et al. - 2022 - Applying machine classifiers to update searches A.pdf;/home/max/Zotero/storage/PIP29UIU/jrsm.html}
}

@article{surkovic_scientific_2022,
  title = {Scientific Advice for Policymakers on Climate Change: The Role of Evidence Synthesis},
  shorttitle = {Scientific Advice for Policymakers on Climate Change},
  author = {Surkovic, Elizabeth and Vigar, David},
  year = {2022},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {380},
  number = {2221},
  pages = {20210147},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2021.0147},
  urldate = {2024-01-15},
  abstract = {Science has a role to play in providing the evidence on both climate change and the solutions to it. In this paper, we look at the nature of expert advice to public policymakers and examine one approach to the synthesis of scientific evidence. We focus on a series of briefings for policymakers that summarize evidence from 12 areas of science and technology which are keys to accelerating progress towards net zero greenhouse gas emissions and resilience to climate change. This article is part of the theme issue `Developing resilient energy systems'.},
  keywords = {climate change,evidence synthesis,policy},
  file = {/home/max/Zotero/storage/D5P4YBPI/Surkovic and Vigar - 2022 - Scientific advice for policymakers on climate chan.pdf}
}

@article{terasawa_systematic_2009,
  title = {Systematic {{Review}}: {{Charged-Particle Radiation Therapy}} for {{Cancer}}},
  shorttitle = {Systematic {{Review}}},
  author = {Terasawa, Teruhiko and Dvorak, Tomas and Ip, Stanley and Raman, Gowri and Lau, Joseph and Trikalinos, Thomas A.},
  year = {2009},
  month = oct,
  journal = {Annals of Internal Medicine},
  volume = {151},
  number = {8},
  pages = {556--565},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-151-8-200910200-00145},
  urldate = {2023-09-08}
}

@misc{touvron_llama_2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2024-01-15},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/max/Zotero/storage/JWHEVXMS/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;/home/max/Zotero/storage/8ZJE4ITL/2302.html}
}

@misc{touvron_llama_2023-1,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2024-11-04},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{touvron_llama_2023-2,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2024-11-04},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{tsafnat_systematic_2014,
  title = {Systematic Review Automation Technologies},
  author = {Tsafnat, Guy and Glasziou, Paul and Choong, Miew Keen and Dunn, Adam and Galgani, Filippo and Coiera, Enrico},
  year = {2014},
  month = jul,
  journal = {Systematic Reviews},
  volume = {3},
  number = {1},
  pages = {74},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-3-74},
  urldate = {2024-05-08},
  abstract = {Systematic reviews, a cornerstone of evidence-based medicine, are not produced quickly enough to support clinical practice. The cost of production, availability of the requisite expertise and timeliness are often quoted as major contributors for the delay. This detailed survey of the state of the art of information systems designed to support or automate individual tasks in the systematic review, and in particular systematic reviews of randomized controlled clinical trials, reveals trends that see the convergence of several parallel research projects.},
  keywords = {Information extraction,Information retrieval,Process automation,Systematic reviews},
  file = {/home/max/Zotero/storage/LNQ87CMR/Tsafnat et al. - 2014 - Systematic review automation technologies.pdf;/home/max/Zotero/storage/NVNRL7CN/2046-4053-3-74.html}
}

@article{van_de_schoot_open_2021,
  title = {An Open Source Machine Learning Framework for Efficient and Transparent Systematic Reviews},
  author = {{van de Schoot}, Rens and {de Bruin}, Jonathan and Schram, Raoul and Zahedi, Parisa and {de Boer}, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
  year = {2021},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {2},
  pages = {125--133},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00287-7},
  urldate = {2022-10-19},
  abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks---including but not limited to systematic reviews and meta-analyses---the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computer science,Medical research,SARS-CoV-2},
  file = {/home/max/Zotero/storage/6W2G29F9/van de Schoot et al. - 2021 - An open source machine learning framework for effi.pdf;/home/max/Zotero/storage/MY58Y7SH/s42256-020-00287-7.html}
}

@article{van_de_schoot_open_2021-1,
  title = {An Open Source Machine Learning Framework for Efficient and Transparent Systematic Reviews},
  author = {{van de Schoot}, Rens and {de Bruin}, Jonathan and Schram, Raoul and Zahedi, Parisa and {de Boer}, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
  year = {2021},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {2},
  pages = {125--133},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00287-7},
  urldate = {2023-10-12},
  abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks---including but not limited to systematic reviews and meta-analyses---the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computer science,Medical research,SARS-CoV-2},
  file = {/home/max/Zotero/storage/3BTFLP6J/van de Schoot et al. - 2021 - An open source machine learning framework for effi.pdf}
}

@article{van_de_schoot_open_2021-2,
  title = {An Open Source Machine Learning Framework for Efficient and Transparent Systematic Reviews},
  author = {{van de Schoot}, Rens and {de Bruin}, Jonathan and Schram, Raoul and Zahedi, Parisa and {de Boer}, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
  year = {2021},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {2},
  pages = {125--133},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00287-7},
  urldate = {2024-01-15},
  abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks---including but not limited to systematic reviews and meta-analyses---the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computer science,Medical research,SARS-CoV-2},
  file = {/home/max/Zotero/storage/DAQM6AGY/van de Schoot et al. - 2021 - An open source machine learning framework for effi.pdf}
}

@misc{walton_covidence_2023,
  title = {Covidence {{Product Updates}} and {{Bug Fixes}}},
  author = {Walton, Alex},
  year = {2023},
  month = jan,
  journal = {Covidence},
  urldate = {2023-10-12},
  abstract = {The latest product updates from Covidence. Find out how machine learning (active learning) is now helping teams move relevant studies through to full-text review sooner, how you can import effect estimates into RevMan 5 and all about the niggling bugs we have fixed.},
  howpublished = {https://www.covidence.org/blog/release-notes-december-2022-machine-learning/},
  langid = {australian},
  file = {/home/max/Zotero/storage/KVUYFZ8Z/release-notes-december-2022-machine-learning.html}
}

@inproceedings{wang_neural_2022,
  title = {Neural {{Rankers}} for {{Effective Screening Prioritisation}} in {{Medical Systematic Review Literature Search}}},
  booktitle = {Proceedings of the 26th {{Australasian Document Computing Symposium}}},
  author = {Wang, Shuai and Scells, Harrisen and Koopman, Bevan and Zuccon, Guido},
  year = {2022},
  month = dec,
  eprint = {2212.09017},
  primaryclass = {cs},
  pages = {1--10},
  doi = {10.1145/3572960.3572980},
  urldate = {2024-02-05},
  abstract = {Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/AF74ICR6/Wang et al. - 2022 - Neural Rankers for Effective Screening Prioritisat.pdf;/home/max/Zotero/storage/6UH834A9/2212.html}
}

@misc{wang_zero-shot_2024,
  title = {Zero-Shot {{Generative Large Language Models}} for {{Systematic Review Screening Automation}}},
  author = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06320},
  eprint = {2401.06320},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-25},
  abstract = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/max/Zotero/storage/G6MSRDK4/Wang et al. - 2024 - Zero-shot Generative Large Language Models for Sys.pdf;/home/max/Zotero/storage/LZCXGJJN/2401.html}
}

@inproceedings{wolf_transformers_2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  year = {2020},
  month = oct,
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  urldate = {2024-01-15},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  file = {/home/max/Zotero/storage/LSX6IELT/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@misc{xia_llmscreen_2024,
  title = {{{LLMscreen}}: {{A Python Package}} for {{Systematic Review Screening}} of {{Scientific Texts Using Prompt Engineering}}},
  shorttitle = {{{LLMscreen}}},
  author = {Xia, Ziqian and Ye, Jinquan and Hu, Bo and Qiang, Qiqi and Debnath, Ramit},
  year = {2024},
  month = sep,
  publisher = {Research Square},
  issn = {2693-5015},
  doi = {10.21203/rs.3.rs-5063165/v1},
  urldate = {2024-10-29},
  abstract = {Systematic reviews represent a cornerstone of evidence-based research, yet the process is labor-intensive and time-consuming, often requiring substantial human resources. The advent of Large Language Models (LLMs) offers a novel approach to streamlining systematic reviews, particularly in the title and abstract screening phase. This study introduces a new Python package built on LLMs to accelerate this process, evaluating its performance across three datasets using distinct prompt strategies: single-prompt, k-value setting, and zero-shot. The k-value setting approach emerged as the most effective, achieving a precision of 0.649 and reducing the average error rate to 0.4\%, significantly lower than the 10.76\% error rate typically observed among human reviewers. Moreover, this approach enabled the screening of 3,000 papers in under 8 minutes, at a cost of only \$0.30\&amp;mdash;an over 250-fold improvement in time and 2,000-fold cost efficiency compared to traditional methods. These findings underscore the potential of LLMs to enhance the efficiency and accuracy of systematic reviews, though further research is needed to address challenges related to dataset variability and model transparency. Expanding the application of LLMs to other stages of systematic reviews, such as data extraction and synthesis, could further streamline the review process, making it more comprehensive and less burdensome for researchers.},
  archiveprefix = {Research Square},
  file = {/home/max/Zotero/storage/DAXUS9I4/Xia et al. - 2024 - LLMscreen A Python Package for Systematic Review .pdf}
}

@misc{yang_goldilocks_2022,
  title = {Goldilocks: {{Just-Right Tuning}} of {{BERT}} for {{Technology-Assisted Review}}},
  shorttitle = {Goldilocks},
  author = {Yang, Eugene and MacAvaney, Sean and Lewis, David D. and Frieder, Ophir},
  year = {2022},
  month = jan,
  number = {arXiv:2105.01044},
  eprint = {2105.01044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.01044},
  urldate = {2024-02-05},
  abstract = {Technology-assisted review (TAR) refers to iterative active learning workflows for document review in high recall retrieval (HRR) tasks. TAR research and most commercial TAR software have applied linear models such as logistic regression to lexical features. Transformer-based models with supervised tuning are known to improve effectiveness on many text classification tasks, suggesting their use in TAR. We indeed find that the pre-trained BERT model reduces review cost by 10\% to 15\% in TAR workflows simulated on the RCV1-v2 newswire collection. In contrast, we likewise determined that linear models outperform BERT for simulated legal discovery topics on the Jeb Bush e-mail collection. This suggests the match between transformer pre-training corpora and the task domain is of greater significance than generally appreciated. Additionally, we show that just-right language model fine-tuning on the task collection before starting active learning is critical. Too little or too much fine-tuning hinders performance, worse than that of linear models, even for a favorable corpus such as RCV1-v2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/max/Zotero/storage/IRU2NDA8/Yang et al. - 2022 - Goldilocks Just-Right Tuning of BERT for Technolo.pdf;/home/max/Zotero/storage/MLP6YXFX/2105.html}
}

@article{yu_fast2_2019,
  title = {{{FAST2}}: {{An}} Intelligent Assistant for Finding Relevant Papers},
  shorttitle = {{{FAST2}}},
  author = {Yu, Zhe and Menzies, Tim},
  year = {2019},
  month = apr,
  journal = {Expert Systems with Applications},
  volume = {120},
  pages = {57--71},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.11.021},
  urldate = {2023-09-08},
  abstract = {Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95\%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers---which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found---which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm---which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.},
  keywords = {Active learning,Literature reviews,Relevance feedback,Selection process,Semi-supervised learning,Text mining},
  file = {/home/max/Zotero/storage/WYK88P2F/Yu and Menzies - 2019 - FAST2 An intelligent assistant for finding releva.pdf;/home/max/Zotero/storage/U76U9I6R/S0957417418307413.html}
}
